{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOotwmTenWYyg6Tv8IOiMNf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52378/Explainable-AI-Lab/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc29mES14FsD",
        "outputId": "54b3792f-ff77-42ec-ba0e-ecfc12f0d620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install kagglehub[pandas-datasets]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openpyxl kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtElMenR4ODu",
        "outputId": "3b053d8d-c6fb-4963-a315-b191a0ca2bbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lung_cancer_analysis.py\n",
        "# Full pipeline: EDA -> Preprocessing -> ML & DL -> XAI\n",
        "# Adjust paths if needed.\n",
        "\n",
        "import os, json, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "out_dir = \"/mnt/data/lung_analysis_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "dataset_path = \"/content/lung_cancer_dataset.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# EDA\n",
        "eda = {\n",
        "    \"shape\": df.shape,\n",
        "    \"dtypes\": df.dtypes.apply(lambda x: str(x)).to_dict(),\n",
        "    \"missing_count\": df.isnull().sum().to_dict(),\n",
        "    \"missing_pct\": (df.isnull().mean()*100).round(3).to_dict()\n",
        "}\n",
        "pd.DataFrame([eda]).T.to_csv(os.path.join(out_dir, \"eda_summary_table.csv\"))\n",
        "df.head(10).to_csv(os.path.join(out_dir, \"head_preview.csv\"), index=False)\n",
        "pd.DataFrame(df.describe(include=[np.number]).T).to_csv(os.path.join(out_dir, \"numeric_summary.csv\"))\n",
        "\n",
        "# Detect target\n",
        "possible_targets = ['target','class','diagnosis','label','result','lung_cancer','cancer','status']\n",
        "found_target = None\n",
        "for col in df.columns:\n",
        "    if col.lower() in possible_targets:\n",
        "        found_target = col; break\n",
        "if not found_target:\n",
        "    found_target = df.columns[-1]\n",
        "target_col = found_target\n",
        "\n",
        "# Class counts\n",
        "class_counts = df[target_col].value_counts(dropna=False)\n",
        "pd.DataFrame(class_counts).to_csv(os.path.join(out_dir, \"class_counts.csv\"))\n",
        "\n",
        "# Numeric histograms + correlation heatmap\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for c in num_cols:\n",
        "    plt.figure(figsize=(6,3))\n",
        "    df[c].hist(bins=30)\n",
        "    plt.title(c)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, f\"hist_{c}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "if len(num_cols) >= 2:\n",
        "    corr = df[num_cols].corr()\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(corr, cmap='coolwarm', interpolation='nearest')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
        "    plt.yticks(range(len(num_cols)), num_cols)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, \"correlation_heatmap.png\"))\n",
        "    corr.to_csv(os.path.join(out_dir, \"correlation_matrix.csv\"))\n",
        "\n",
        "# Preprocessing\n",
        "X = df.drop(columns=[target_col]).copy()\n",
        "y = df[target_col].copy()\n",
        "\n",
        "# Categorical encoding\n",
        "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
        "for c in cat_cols:\n",
        "    X[c] = X[c].astype(str).fillna(\"MISSING\")\n",
        "    le = LabelEncoder(); X[c] = le.fit_transform(X[c])\n",
        "\n",
        "# Numeric impute + scale\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X[num_cols] = imputer.fit_transform(X[num_cols])\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Target encode if necessary\n",
        "if y.dtype == 'object' or y.dtype.name == 'category':\n",
        "    y = LabelEncoder().fit_transform(y.astype(str))\n",
        "else:\n",
        "    unique_vals = np.unique(y.dropna())\n",
        "    if len(unique_vals) == 2:\n",
        "        mapping = {unique_vals[0]:0, unique_vals[1]:1}\n",
        "        y = y.map(mapping)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y))>1 else None)\n",
        "\n",
        "# SMOTE (if imblearn installed and necessary)\n",
        "smote_applied = False\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    from collections import Counter\n",
        "    ctr = Counter(y_train)\n",
        "    maj = max(ctr.values()); minv = min(ctr.values())\n",
        "    if len(ctr) == 2 and (minv/maj < 0.6):\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "        smote_applied = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Models\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
        "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(probability=True, random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "}\n",
        "\n",
        "# XGBoost optional\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    models['XGBoost'] = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else None\n",
        "        res = {\n",
        "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "            \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "            \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
        "            \"confusion_matrix\": confusion_matrix(y_test, y_pred).tolist()\n",
        "        }\n",
        "        if y_proba is not None:\n",
        "            res['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
        "        results[name] = res\n",
        "    except Exception as e:\n",
        "        results[name] = {\"error\": str(e)}\n",
        "\n",
        "pd.DataFrame(results).T.to_csv(os.path.join(out_dir, \"ml_results.csv\"))\n",
        "\n",
        "# (Optional) Deep learning with tensorflow - run only if tensorflow available\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.layers import Dense, Conv1D, Flatten, Input, LSTM\n",
        "    X_train_np = np.array(X_train).astype('float32'); X_test_np = np.array(X_test).astype('float32')\n",
        "    y_train_np = np.array(y_train).astype('float32'); y_test_np = np.array(y_test).astype('float32')\n",
        "\n",
        "    # MLP\n",
        "    mlp = Sequential([Dense(64, activation='relu', input_shape=(X_train_np.shape[1],)),\n",
        "                     Dense(32, activation='relu'),\n",
        "                     Dense(1, activation='sigmoid')])\n",
        "    mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    mlp.fit(X_train_np, y_train_np, epochs=10, batch_size=32, verbose=1)\n",
        "    y_prob = mlp.predict(X_test_np).ravel(); y_pred = (y_prob>0.5).astype(int)\n",
        "    # compute and save metrics ...\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Feature importance via RandomForest\n",
        "try:\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    importances = rf.feature_importances_\n",
        "    feat_imp = pd.DataFrame({\"feature\": X.columns, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
        "    feat_imp.to_csv(os.path.join(out_dir, \"feature_importances.csv\"), index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Short report\n",
        "report_md = f\"\"\"\n",
        "# Auto-generated Lung Cancer Analysis Report\n",
        "Dataset: {dataset_path}\n",
        "Rows,Cols: {df.shape}\n",
        "Detected target: {target_col}\n",
        "SMOTE applied: {smote_applied}\n",
        "\"\"\"\n",
        "with open(os.path.join(out_dir, \"report.md\"), \"w\") as f:\n",
        "    f.write(report_md)\n",
        "print(\"Finished. Outputs in:\", out_dir)\n"
      ],
      "metadata": {
        "id": "XTLmKpG34Vj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a ZIP file of the outputs folder\n",
        "shutil.make_archive('/mnt/data/lung_analysis_outputs_zip', 'zip', '/mnt/data/lung_analysis_outputs')"
      ],
      "metadata": {
        "id": "tfZbX78l6BTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/mnt/data/lung_analysis_outputs_zip.zip')"
      ],
      "metadata": {
        "id": "j9xyBgYg6Hf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}